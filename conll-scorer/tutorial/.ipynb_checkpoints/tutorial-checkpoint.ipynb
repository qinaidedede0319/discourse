{
 "metadata": {
  "name": "",
  "signature": "sha256:6da9e549a8c559312806fc209020cfaa986b8317a788e0af5f231941f5087875"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# CoNLL 2016 Shared Task Data Format\n",
      "\n",
      "The data format is identical to what we used last year. But we made slight changes to some of the file names in the package to prevent confusion from last year. The package name indicates language (en or zh) and the date of creation (MM-DD-YY) and the data split (train, dev, trial, etc). Once you unpack the package, you can expect the following files and folders:\n",
      "\n",
      "* `parses.json` - The input file for the main task and the supplementary task (`pdtb-parses.json` in 2015)\n",
      "* `relations-no-senses.json` - The input file for the supplementary task (new this year)\n",
      "* `relations.json` - the gold standard discourse relations (`pdtb-data.json` in 2015)\n",
      "* `raw/DocID` - plain text file. One file per document. No extension. File name will match the DocID field in relations.json and key in parses.json.\n",
      "* `conll_format/DocID.conll` - CoNLL format for the training data (one file per document .conll)\n",
      "\n",
      "We will show you how to work with each of these files in order to train your systems for the main task and the supplementary in the language of your choice. \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls -l conll16st-en-01-12-16-trial"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "total 496\r\n",
        "drwxr-xr-x+ 3 te  staff     102 Jan 12 09:42 \u001b[34mconll_format\u001b[m\u001b[m/\r\n",
        "-rw-r--r--+ 1 te  staff    9950 Jan 13 11:42 output.json\r\n",
        "-rw-r--r--+ 1 te  staff  150222 Jan 12 09:40 parses.json\r\n",
        "drwxr-xr-x+ 3 te  staff     102 Jan 12 09:42 \u001b[34mraw\u001b[m\u001b[m/\r\n",
        "-rw-r--r--+ 1 te  staff   41739 Jan 12 09:42 relations-no-senses.json\r\n",
        "-rw-r--r--+ 1 te  staff   42610 Jan 12 09:40 relations.json\r\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`relations.json` : Gold standard discourse relation annotation\n",
      "-----------------------------------\n",
      "This file is from The Penn Discourse Treebank (PDTB) & Chinese Discourse Treebank (CDTB) for English and Chinese respectively.\n",
      "These are the gold standard annotation for both the main task and the supplementary task. Each line in the file is a json line. In Python, you can turn it into a dictionary. Similarly, you can turn it into HashMap in Java. But please do not do not use regex to parse json. Your system will most likely break during evaluation. \n",
      "\n",
      "The dictionary describes the following component of a relation:\n",
      "\n",
      "* `Arg1` : the text span of Arg1 of the relation\n",
      "* `Arg2` : the text span of Arg2 of the relation\n",
      "* `Connective` : the text span of the connective of the relation\n",
      "* `DocID` : document id where the relation is in.\n",
      "* `ID` : the relation id, which is unique across training, dev, and test sets.\n",
      "* `Sense` : the sense of the relation \n",
      "* `Type` : the type of relation (Explicit, Implicit, Entrel, AltLex, or NoRel)\n",
      "\n",
      "The text span is in the same format for `Arg1`, `Arg2`, and `Connective`. A text span has the following fields:\n",
      "\n",
      "* `CharacterSpanList` : the list of character offsets (beginning, end) in the raw untokenized data file. \n",
      "* `RawText` : the raw untokenized text of the span\n",
      "* `TokenList` : the list of the addresses of the tokens in the form of \n",
      "(character offset begin, character offset end, token offset within the document, sentence offset, token offset within the sentence)\n",
      "\n",
      "For example, "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import codecs\n",
      "pdtb_file = codecs.open('conll16st-en-01-12-16-trial/relations.json', encoding='utf8')\n",
      "relations = [json.loads(x) for x in pdtb_file];\n",
      "example_relation = relations[10]\n",
      "example_relation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "{u'Arg1': {u'CharacterSpanList': [[2493, 2517]],\n",
        "  u'RawText': u'and told them to cool it',\n",
        "  u'TokenList': [[2493, 2496, 465, 15, 8],\n",
        "   [2497, 2501, 466, 15, 9],\n",
        "   [2502, 2506, 467, 15, 10],\n",
        "   [2507, 2509, 468, 15, 11],\n",
        "   [2510, 2514, 469, 15, 12],\n",
        "   [2515, 2517, 470, 15, 13]]},\n",
        " u'Arg2': {u'CharacterSpanList': [[2526, 2552]],\n",
        "  u'RawText': u\"they're ruining the market\",\n",
        "  u'TokenList': [[2526, 2530, 472, 15, 15],\n",
        "   [2530, 2533, 473, 15, 16],\n",
        "   [2534, 2541, 474, 15, 17],\n",
        "   [2542, 2545, 475, 15, 18],\n",
        "   [2546, 2552, 476, 15, 19]]},\n",
        " u'Connective': {u'CharacterSpanList': [[2518, 2525]],\n",
        "  u'RawText': u'because',\n",
        "  u'TokenList': [[2518, 2525, 471, 15, 14]]},\n",
        " u'DocID': u'wsj_1000',\n",
        " u'ID': 14887,\n",
        " u'Sense': [u'Contingency.Cause.Reason'],\n",
        " u'Type': u'Explicit'}"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Differences in Chinese data\n",
      "--------------------------\n",
      "Everything in Chinese data and English data are identical except that Chinese data have one extra field `Punctuation`. Punctuations in Chinese have some discourse functions, so they are annotated as well. But you are not required to detect those as part of the task. Discourse annotation in Chinese differs quite a bit from English from the linguistics perspective. Please refer to [the original paper in Chinese Discourse Treebank](http://www.aclweb.org/anthology/P12-1008.pdf). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = codecs.open('conll16st-zh-01-08-2016-trial/relations.json', encoding='utf8')\n",
      "chinese_relations = [json.loads(x) for x in data]\n",
      "chinese_relations[13]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "{u'Arg1': {u'CharacterSpanList': [[500, 511]],\n",
        "  u'RawText': u'\\u6210\\u4ea4 \\u836f\\u54c1 \\u4e00\\u4ebf\\u591a \\u5143',\n",
        "  u'TokenList': [[500, 502, 187, 5, 27],\n",
        "   [503, 505, 188, 5, 28],\n",
        "   [506, 509, 189, 5, 29],\n",
        "   [510, 511, 190, 5, 30]]},\n",
        " u'Arg2': {u'CharacterSpanList': [[514, 526]],\n",
        "  u'RawText': u'\\u6ca1\\u6709 \\u53d1\\u73b0 \\u4e00 \\u4f8b \\u56de\\u6263',\n",
        "  u'TokenList': [[514, 516, 192, 5, 32],\n",
        "   [517, 519, 193, 5, 33],\n",
        "   [520, 521, 194, 5, 34],\n",
        "   [522, 523, 195, 5, 35],\n",
        "   [524, 526, 196, 5, 36]]},\n",
        " u'Connective': {u'CharacterSpanList': [], u'RawText': u'', u'TokenList': []},\n",
        " u'DocID': u'chtb_0001',\n",
        " u'ID': 13,\n",
        " u'Punctuation': {u'CharacterSpanList': [[512, 513]],\n",
        "  u'PunctuationType': u'Comma',\n",
        "  u'RawText': u'\\uff0c',\n",
        "  u'TokenList': [[512, 513, 191, 5, 31]]},\n",
        " u'Sense': [u'Conjunction'],\n",
        " u'Type': u'Implicit'}"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Arg1 : %s\\nArg2 : %s' % (chinese_relations[13]['Arg1']['RawText'], chinese_relations[13]['Arg2']['RawText'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Arg1 : \u6210\u4ea4 \u836f\u54c1 \u4e00\u4ebf\u591a \u5143\n",
        "Arg2 : \u6ca1\u6709 \u53d1\u73b0 \u4e00 \u4f8b \u56de\u6263\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`parses.json` : Input for the main task and the supplementary task\n",
      "--------------------------\n",
      "This is the file that your system will have to process during evaluation. \n",
      "The automatic parses and part-of-speech tags are provided in this file.\n",
      "Note that this file contains only one line unlike the discourse relation json file.\n",
      "Suppose we want the parse for the sentence in the relation above, which is sentence #15 shown in `TokenList`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parse_file = codecs.open('conll16st-en-01-12-16-trial/parses.json', encoding='utf8')\n",
      "en_parse_dict = json.load(parse_file)\n",
      "\n",
      "en_example_relation = relations[10]\n",
      "en_doc_id = en_example_relation['DocID']\n",
      "print en_parse_dict[en_doc_id]['sentences'][15]['parsetree']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "( (S (NP (PRP We)) (VP (VBP 've) (VP (VP (VBN talked) (PP (TO to) (NP (NP (NNS proponents)) (PP (IN of) (NP (NN index) (NN arbitrage)))))) (CC and) (VP (VBD told) (NP (PRP them)) (S (VP (TO to) (VP (VB cool) (NP (PRP it)) (SBAR (IN because) (S (NP (PRP they)) (VP (VBP 're) (VP (VBG ruining) (NP (DT the) (NN market)))))))))))) (. .)) )\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parse_file = codecs.open('conll16st-zh-01-08-2016-trial/parses.json', encoding='utf8')\n",
      "zh_parse_dict = json.load(parse_file)\n",
      "\n",
      "zh_example_relation = chinese_relations[13]\n",
      "zh_doc_id = zh_example_relation['DocID']\n",
      "print zh_parse_dict[zh_doc_id]['sentences'][5]['parsetree']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "( (IP (NP (CP (IP (LCP (NP (NT \u53bb\u5e74)) (LC \u521d)) (NP (NP (NR \u6d66\u4e1c)) (NP (NN \u65b0\u533a))) (VP (VV \u8bde\u751f))) (DEC \u7684)) (NP (NP (NR \u4e2d\u56fd)) (QP (OD \u7b2c\u4e00) (CLP (M \u5bb6))) (NP (NN \u533b\u7597) (NN \u673a\u6784))) (NP (NN \u836f\u54c1) (NN \u91c7\u8d2d) (NN \u670d\u52a1) (NN \u4e2d\u5fc3))) (PU \uff0c) (VP (VP (PP (ADVP (AD \u6b63)) (PP (P \u56e0\u4e3a) (IP (IP (VP (ADVP (AD \u4e00)) (VP (VV \u5f00\u59cb)))) (VP (ADVP (AD \u5c31)) (ADVP (AD \u6bd4\u8f83)) (VP (VA \u89c4\u8303)))))) (PU \uff0c) (VP (VV \u8fd0\u8f6c) (IP (VP (ADVP (AD \u81f3\u4eca)) (PU \uff0c) (VP (VV \u6210\u4ea4) (NP (NN \u836f\u54c1)) (QP (CD \u4e00\u4ebf\u591a) (CLP (M \u5143)))))))) (PU \uff0c) (VP (ADVP (AD \u6ca1\u6709)) (VP (VV \u53d1\u73b0) (NP (QP (CD \u4e00) (CLP (M \u4f8b))) (NP (NN \u56de\u6263)))))) (PU \u3002)) )\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "en_parse_dict[en_doc_id]['sentences'][15]['dependencies']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[[u'nsubj', u'talked-3', u'We-1'],\n",
        " [u'aux', u'talked-3', u\"'ve-2\"],\n",
        " [u'root', u'ROOT-0', u'talked-3'],\n",
        " [u'prep', u'talked-3', u'to-4'],\n",
        " [u'pobj', u'to-4', u'proponents-5'],\n",
        " [u'prep', u'proponents-5', u'of-6'],\n",
        " [u'nn', u'arbitrage-8', u'index-7'],\n",
        " [u'pobj', u'of-6', u'arbitrage-8'],\n",
        " [u'cc', u'talked-3', u'and-9'],\n",
        " [u'conj', u'talked-3', u'told-10'],\n",
        " [u'dobj', u'told-10', u'them-11'],\n",
        " [u'aux', u'cool-13', u'to-12'],\n",
        " [u'xcomp', u'told-10', u'cool-13'],\n",
        " [u'dobj', u'cool-13', u'it-14'],\n",
        " [u'mark', u'ruining-18', u'because-15'],\n",
        " [u'nsubj', u'ruining-18', u'they-16'],\n",
        " [u'aux', u'ruining-18', u\"'re-17\"],\n",
        " [u'advcl', u'cool-13', u'ruining-18'],\n",
        " [u'det', u'market-20', u'the-19'],\n",
        " [u'dobj', u'ruining-18', u'market-20']]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each token can be iterated from `words` field within the sentence. Note that `Linkers` field is provided to indicate whether that token is part of an Arg or not. The format is `arg1_ID`. The ID corresponds to the ID field in the relation json."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "en_parse_dict[en_doc_id]['sentences'][15]['words'][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[u'We',\n",
        " {u'CharacterOffsetBegin': 2447,\n",
        "  u'CharacterOffsetEnd': 2449,\n",
        "  u'Linkers': [u'arg2_14886', u'arg1_14888'],\n",
        "  u'PartOfSpeech': u'PRP'}]"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "en_parse_dict[en_doc_id]['sentences'][15]['words'][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[u\"'ve\",\n",
        " {u'CharacterOffsetBegin': 2449,\n",
        "  u'CharacterOffsetEnd': 2452,\n",
        "  u'Linkers': [u'arg2_14886', u'arg1_14888'],\n",
        "  u'PartOfSpeech': u'VBP'}]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`relations-no-senses.json` : Input for the supplementary task\n",
      "--------------------------\n",
      "The systems participating in the supplementary task (sense classification) take in this file as input. The file is the same as `relations.json` but the `Type` and `Sense` fields are left empty. This is the same for Chinese and English except for the `Punctuation` field. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "supp_data = open('conll16st-en-01-12-16-trial/relations-no-senses.json')\n",
      "relations_no_senses = [json.loads(x) for x in supp_data]\n",
      "relations_no_senses[10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "{u'Arg1': {u'CharacterSpanList': [[2493, 2517]],\n",
        "  u'RawText': u'and told them to cool it',\n",
        "  u'TokenList': [[2493, 2496, 465, 15, 8],\n",
        "   [2497, 2501, 466, 15, 9],\n",
        "   [2502, 2506, 467, 15, 10],\n",
        "   [2507, 2509, 468, 15, 11],\n",
        "   [2510, 2514, 469, 15, 12],\n",
        "   [2515, 2517, 470, 15, 13]]},\n",
        " u'Arg2': {u'CharacterSpanList': [[2526, 2552]],\n",
        "  u'RawText': u\"they're ruining the market\",\n",
        "  u'TokenList': [[2526, 2530, 472, 15, 15],\n",
        "   [2530, 2533, 473, 15, 16],\n",
        "   [2534, 2541, 474, 15, 17],\n",
        "   [2542, 2545, 475, 15, 18],\n",
        "   [2546, 2552, 476, 15, 19]]},\n",
        " u'Connective': {u'CharacterSpanList': [[2518, 2525]],\n",
        "  u'RawText': u'because',\n",
        "  u'TokenList': [[2518, 2525, 471, 15, 14]]},\n",
        " u'DocID': u'wsj_1000',\n",
        " u'ID': 14887,\n",
        " u'Sense': [],\n",
        " u'Type': u''}"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"The CoNLL Format\" \n",
      "--------------------------\n",
      "JSON format makes your code much more readable instead of a bunch of unreadable indices. \n",
      "CoNLL format of this dataset is wicked sparse. Here's our suggested way to get something similar.\n",
      "You can use the `Linker` field in each token dictionary. Here's an example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_tokens = [token for sentence in en_parse_dict[en_doc_id]['sentences'] for token in sentence['words']]\n",
      "for token in all_tokens[0:20]:\n",
      "    for linker in token[1]['Linkers']:\n",
      "        role, relation_id = linker.split('_')\n",
      "        print '%s \\t is part of %s in relation id %s' % (token[0], role, relation_id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Kemper \t is part of arg1 in relation id 14890\n",
        "Financial \t is part of arg1 in relation id 14890\n",
        "Services \t is part of arg1 in relation id 14890\n",
        "Inc. \t is part of arg1 in relation id 14890\n",
        ", \t is part of arg1 in relation id 14890\n",
        "charging \t is part of arg1 in relation id 14890\n",
        "that \t is part of arg1 in relation id 14890\n",
        "program \t is part of arg1 in relation id 14890\n",
        "trading \t is part of arg1 in relation id 14890\n",
        "is \t is part of arg1 in relation id 14890\n",
        "ruining \t is part of arg1 in relation id 14890\n",
        "the \t is part of arg1 in relation id 14890\n",
        "stock \t is part of arg1 in relation id 14890\n",
        "market \t is part of arg1 in relation id 14890\n",
        ", \t is part of arg1 in relation id 14890\n",
        "cut \t is part of arg1 in relation id 14890\n",
        "off \t is part of arg1 in relation id 14890\n",
        "four \t is part of arg1 in relation id 14890\n",
        "big \t is part of arg1 in relation id 14890\n",
        "Wall \t is part of arg1 in relation id 14890\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Relation ID is %s' % relations[13]['ID']\n",
      "print 'Arg 1 : %s' % relations[13]['Arg1']['RawText']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Relation ID is 14890\n",
        "Arg 1 : Kemper Financial Services Inc., charging that program trading is ruining the stock market, cut off four big Wall Street firms from doing any of its stock-trading business\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also provide CoNLL format for those who prefer it but it does not very pretty. Those can also be used for training. CoNLL format will not be provided during evaluation. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for x in open('conll16st-en-01-12-16-trial/conll_format/wsj_1000.conll').readlines()[0:5]:\n",
      "    print x[0:40]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\t0\t0\tKemper\tNNP\targ1\t_\t_\t_\t_\t_\t_\t_\t_\t_\t\n",
        "1\t0\t1\tFinancial\tNNP\targ1\t_\t_\t_\t_\t_\t_\t_\t_\n",
        "2\t0\t2\tServices\tNNPS\targ1\t_\t_\t_\t_\t_\t_\t_\t_\n",
        "3\t0\t3\tInc.\tNNP\targ1\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t\n",
        "4\t0\t4\t,\t,\targ1\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\t_\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's the explanation of each field if a document has n relations:\n",
      "\n",
      "* Document-level token index \n",
      "* Sentence index\n",
      "* Sentence-level token index\n",
      "* POS tag\n",
      "* Relation 1 information\n",
      "* Relation 2 information \n",
      "* ...\n",
      "* Relation n information\n",
      "\n",
      "The relation information field can take many forms:\n",
      "\n",
      "* `arg1` part of Arg1 of the relation\n",
      "* `arg2` part of Arg2 of the relation\n",
      "* `conn|Comparison.Concession` part of the discourse connective AND the sense of that relation is Comparison.Concession (Explicit relations only)\n",
      "* `arg2|EntRel` part of Arg2 of the relation AND the sense of that relation is EntRel (Entrel and Norel relations only)\n",
      "* `arg2|because|Contingency.Pragmatic cause` part of Arg2 (Implicit relations only)\n",
      "\n",
      "What should the system output look like? \n",
      "-----------------------------------------------------\n",
      "The system output must be in json format. It is very similar to the training set except for the `TokenList` field. \n",
      "The `TokenList` field is now a list of document level token indices.\n",
      "If the relation is not explicit, `Connective` field must still be there, and its `TokenList` must be an empty list.\n",
      "You may however add whatever field into json to help yourself debug or develop the system.\n",
      "Below is an example of a relation given by a system.\n",
      "\n",
      "You can also run the sample parser:\n",
      "\n",
      "`python sample_parser.py conll16st-en-01-12-16-trial inputrun tutorial`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output_relations = [json.loads(x) for x in codecs.open('output.json', encoding='utf8')]\n",
      "output_relations[10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "{u'Arg1': {u'TokenList': [275,\n",
        "   276,\n",
        "   277,\n",
        "   278,\n",
        "   279,\n",
        "   280,\n",
        "   281,\n",
        "   282,\n",
        "   283,\n",
        "   284,\n",
        "   285,\n",
        "   286,\n",
        "   287,\n",
        "   288,\n",
        "   289,\n",
        "   290,\n",
        "   291,\n",
        "   292,\n",
        "   293,\n",
        "   294,\n",
        "   295,\n",
        "   296,\n",
        "   297,\n",
        "   298,\n",
        "   299,\n",
        "   300,\n",
        "   301,\n",
        "   302,\n",
        "   303,\n",
        "   304,\n",
        "   305,\n",
        "   306,\n",
        "   307,\n",
        "   308,\n",
        "   309,\n",
        "   310,\n",
        "   311,\n",
        "   312,\n",
        "   313,\n",
        "   314,\n",
        "   315,\n",
        "   316,\n",
        "   317,\n",
        "   318,\n",
        "   319,\n",
        "   320,\n",
        "   321,\n",
        "   322,\n",
        "   323,\n",
        "   324,\n",
        "   325,\n",
        "   326,\n",
        "   327]},\n",
        " u'Arg2': {u'TokenList': [329,\n",
        "   330,\n",
        "   331,\n",
        "   332,\n",
        "   333,\n",
        "   334,\n",
        "   335,\n",
        "   336,\n",
        "   337,\n",
        "   338,\n",
        "   339,\n",
        "   340,\n",
        "   341,\n",
        "   342,\n",
        "   343,\n",
        "   344,\n",
        "   345,\n",
        "   346,\n",
        "   347,\n",
        "   348,\n",
        "   349,\n",
        "   350,\n",
        "   351,\n",
        "   352,\n",
        "   353,\n",
        "   354,\n",
        "   355,\n",
        "   356,\n",
        "   357,\n",
        "   358,\n",
        "   359,\n",
        "   360,\n",
        "   361,\n",
        "   362,\n",
        "   363]},\n",
        " u'Connective': {u'TokenList': []},\n",
        " u'DocID': u'wsj_1000',\n",
        " u'Sense': [u'Expansion.Conjunction'],\n",
        " u'Type': u'Implicit'}"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Validator and scorer\n",
      "----------------------------\n",
      "Suppose you already have a system and you want to evaluate the system. \n",
      "We provide `validator.py` and `scorer.py` to help you validate the format of the system out and evaluate the system respectively. \n",
      "These utility functions can be downloaded from [CoNLL Shared Task Github](www.github.com/attapol/conll16st).\n",
      "The usage is included in the functions. \n",
      "\n",
      "That should be all that you need! Let's get the fun started.\n",
      "----------------------------\n",
      "If you find any errors or suggestions, please post to the forum or email the organizing committee at `conll16st@gmail.com`. \n",
      "We hope you enjoy solving this challenging task of shallow discourse parsing. \n",
      "Together, we can make progress in understanding discourse phenomena."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}